model = tf.keras.Sequential([
    Bidirectional(LSTM(128, return_sequences=True)),
    Dropout(0.2),
    LSTM(32),
    Dropout(0.2),
    Dense(1),
])

/home/ncclab/anaconda3/envs/tensorflow/bin/python /home/ncclab/github/SEEG-Oddball/code/run_lstm.py
2020-10-26 10:00:49.780102: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-10-26 10:00:49.822187: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2300000000 Hz
2020-10-26 10:00:49.825597: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ce5a7022e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-10-26 10:00:49.825626: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-10-26 10:00:49.826234: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
Epoch 1/50
WARNING:tensorflow:Layer bidirectional is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

5/5 [==============================] - 4s 817ms/step - loss: 6.2553 - accuracy: 0.4786 - val_loss: 2.7767 - val_accuracy: 0.5278
Epoch 2/50
5/5 [==============================] - 3s 517ms/step - loss: 1.6892 - accuracy: 0.5571 - val_loss: 1.6070 - val_accuracy: 0.5833
Epoch 3/50
5/5 [==============================] - 3s 594ms/step - loss: 1.0143 - accuracy: 0.7143 - val_loss: 1.1569 - val_accuracy: 0.5000
Epoch 4/50
5/5 [==============================] - 3s 599ms/step - loss: 0.6178 - accuracy: 0.8000 - val_loss: 1.4033 - val_accuracy: 0.6111
Epoch 5/50
5/5 [==============================] - 3s 545ms/step - loss: 0.4934 - accuracy: 0.7857 - val_loss: 0.9841 - val_accuracy: 0.6389
Epoch 6/50
5/5 [==============================] - 3s 647ms/step - loss: 0.4856 - accuracy: 0.8714 - val_loss: 0.9241 - val_accuracy: 0.7500
Epoch 7/50
5/5 [==============================] - 4s 755ms/step - loss: 0.2687 - accuracy: 0.9071 - val_loss: 1.6166 - val_accuracy: 0.8056
Epoch 8/50
5/5 [==============================] - 3s 622ms/step - loss: 0.2377 - accuracy: 0.9143 - val_loss: 1.3568 - val_accuracy: 0.7500
Epoch 9/50
5/5 [==============================] - 3s 586ms/step - loss: 0.1976 - accuracy: 0.9214 - val_loss: 1.2460 - val_accuracy: 0.7222
Epoch 10/50
5/5 [==============================] - 3s 576ms/step - loss: 0.1535 - accuracy: 0.9643 - val_loss: 1.1466 - val_accuracy: 0.8056
Epoch 11/50
5/5 [==============================] - 3s 578ms/step - loss: 0.2521 - accuracy: 0.9429 - val_loss: 1.1223 - val_accuracy: 0.8333
Epoch 12/50
5/5 [==============================] - 3s 541ms/step - loss: 0.1084 - accuracy: 0.9500 - val_loss: 1.1819 - val_accuracy: 0.7500
Epoch 13/50
5/5 [==============================] - 3s 631ms/step - loss: 0.0945 - accuracy: 0.9786 - val_loss: 1.1760 - val_accuracy: 0.7778
Epoch 14/50
5/5 [==============================] - 3s 518ms/step - loss: 0.0935 - accuracy: 0.9857 - val_loss: 0.8288 - val_accuracy: 0.7500
Epoch 15/50
5/5 [==============================] - 3s 557ms/step - loss: 0.0879 - accuracy: 0.9714 - val_loss: 0.8993 - val_accuracy: 0.6944
Epoch 16/50
5/5 [==============================] - 3s 529ms/step - loss: 0.0755 - accuracy: 0.9571 - val_loss: 1.1515 - val_accuracy: 0.7500
Epoch 17/50
5/5 [==============================] - 3s 502ms/step - loss: 0.0500 - accuracy: 0.9857 - val_loss: 1.1723 - val_accuracy: 0.7222
Epoch 18/50
5/5 [==============================] - 2s 497ms/step - loss: 0.0632 - accuracy: 0.9857 - val_loss: 1.1323 - val_accuracy: 0.7778
Epoch 19/50
5/5 [==============================] - 3s 551ms/step - loss: 0.0425 - accuracy: 0.9857 - val_loss: 1.2064 - val_accuracy: 0.8056
Epoch 20/50
5/5 [==============================] - 3s 532ms/step - loss: 0.0436 - accuracy: 0.9929 - val_loss: 0.7635 - val_accuracy: 0.8333
Epoch 21/50
5/5 [==============================] - 3s 542ms/step - loss: 0.0410 - accuracy: 0.9786 - val_loss: 1.0952 - val_accuracy: 0.8333
Epoch 22/50
5/5 [==============================] - 3s 670ms/step - loss: 0.0311 - accuracy: 1.0000 - val_loss: 1.1094 - val_accuracy: 0.8333
Epoch 23/50
5/5 [==============================] - 3s 508ms/step - loss: 0.0328 - accuracy: 0.9929 - val_loss: 1.0771 - val_accuracy: 0.8333
Epoch 24/50
5/5 [==============================] - 3s 535ms/step - loss: 0.0288 - accuracy: 0.9929 - val_loss: 1.1346 - val_accuracy: 0.8056
Epoch 25/50
5/5 [==============================] - 3s 532ms/step - loss: 0.0248 - accuracy: 1.0000 - val_loss: 1.1615 - val_accuracy: 0.8333
Epoch 26/50
5/5 [==============================] - 3s 527ms/step - loss: 0.0182 - accuracy: 1.0000 - val_loss: 1.5289 - val_accuracy: 0.7778
Epoch 27/50
5/5 [==============================] - 3s 609ms/step - loss: 0.0396 - accuracy: 0.9929 - val_loss: 1.5340 - val_accuracy: 0.7500
Epoch 28/50
5/5 [==============================] - 3s 562ms/step - loss: 0.0240 - accuracy: 0.9929 - val_loss: 1.1972 - val_accuracy: 0.8056
Epoch 29/50
5/5 [==============================] - 3s 501ms/step - loss: 0.0177 - accuracy: 1.0000 - val_loss: 1.4853 - val_accuracy: 0.8056
Epoch 30/50
5/5 [==============================] - 3s 534ms/step - loss: 0.0181 - accuracy: 0.9929 - val_loss: 1.4679 - val_accuracy: 0.8333
Epoch 31/50
5/5 [==============================] - 3s 595ms/step - loss: 0.0202 - accuracy: 1.0000 - val_loss: 1.4664 - val_accuracy: 0.8333
Epoch 32/50
5/5 [==============================] - 3s 515ms/step - loss: 0.0231 - accuracy: 0.9929 - val_loss: 1.4743 - val_accuracy: 0.8333
Epoch 33/50
5/5 [==============================] - 3s 515ms/step - loss: 0.0234 - accuracy: 0.9929 - val_loss: 1.5033 - val_accuracy: 0.8056
Epoch 34/50
5/5 [==============================] - 3s 530ms/step - loss: 0.0375 - accuracy: 0.9929 - val_loss: 1.4692 - val_accuracy: 0.8056
Epoch 35/50
5/5 [==============================] - 3s 528ms/step - loss: 0.0172 - accuracy: 0.9929 - val_loss: 1.5458 - val_accuracy: 0.7500
Epoch 36/50
5/5 [==============================] - 3s 532ms/step - loss: 0.0307 - accuracy: 0.9857 - val_loss: 1.4953 - val_accuracy: 0.7500
Epoch 37/50
5/5 [==============================] - 3s 527ms/step - loss: 0.0231 - accuracy: 0.9929 - val_loss: 1.4808 - val_accuracy: 0.8333
Epoch 38/50
5/5 [==============================] - 3s 501ms/step - loss: 0.0191 - accuracy: 1.0000 - val_loss: 1.5312 - val_accuracy: 0.8333
Epoch 39/50
5/5 [==============================] - 3s 529ms/step - loss: 0.0178 - accuracy: 1.0000 - val_loss: 1.9149 - val_accuracy: 0.8056
Epoch 40/50
5/5 [==============================] - 3s 510ms/step - loss: 0.0170 - accuracy: 0.9929 - val_loss: 1.7084 - val_accuracy: 0.6944
Epoch 41/50
5/5 [==============================] - 3s 538ms/step - loss: 0.0151 - accuracy: 1.0000 - val_loss: 1.5471 - val_accuracy: 0.7500
Epoch 42/50
5/5 [==============================] - 3s 546ms/step - loss: 0.0212 - accuracy: 0.9929 - val_loss: 1.5482 - val_accuracy: 0.8333
Epoch 43/50
5/5 [==============================] - 2s 489ms/step - loss: 0.0290 - accuracy: 0.9929 - val_loss: 1.5530 - val_accuracy: 0.8056
Epoch 44/50
5/5 [==============================] - 3s 543ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 1.6642 - val_accuracy: 0.6944
Epoch 45/50
5/5 [==============================] - 3s 515ms/step - loss: 0.0207 - accuracy: 0.9929 - val_loss: 1.9973 - val_accuracy: 0.6944
Epoch 46/50
5/5 [==============================] - 2s 492ms/step - loss: 0.0321 - accuracy: 0.9929 - val_loss: 1.9683 - val_accuracy: 0.6944
Epoch 47/50
5/5 [==============================] - 3s 537ms/step - loss: 0.1573 - accuracy: 0.9786 - val_loss: 1.7449 - val_accuracy: 0.6944
Epoch 48/50
5/5 [==============================] - 3s 511ms/step - loss: 0.1696 - accuracy: 0.9571 - val_loss: 1.9288 - val_accuracy: 0.7500
Epoch 49/50
5/5 [==============================] - 3s 505ms/step - loss: 0.1839 - accuracy: 0.9643 - val_loss: 2.3100 - val_accuracy: 0.7500
Epoch 50/50
5/5 [==============================] - 3s 529ms/step - loss: 0.0477 - accuracy: 0.9857 - val_loss: 2.4272 - val_accuracy: 0.7500
{'loss': [6.2552947998046875, 1.6891671419143677, 1.0143343210220337, 0.6177664399147034, 0.4934127926826477, 0.48555228114128113, 0.26873132586479187, 0.23768950998783112, 0.19761161506175995, 0.15351024270057678, 0.2520861029624939, 0.10837535560131073, 0.09454625844955444, 0.09352438896894455, 0.08790674060583115, 0.07546675205230713, 0.04995695874094963, 0.06321975588798523, 0.04245882108807564, 0.04356100782752037, 0.041044868528842926, 0.031080041080713272, 0.0328044593334198, 0.028767528012394905, 0.024761701002717018, 0.018213247880339622, 0.03958509489893913, 0.02401377260684967, 0.01773097738623619, 0.01810384728014469, 0.02020338922739029, 0.023117072880268097, 0.02343137003481388, 0.03754177317023277, 0.017161032184958458, 0.030686484649777412, 0.023083867505192757, 0.019146820530295372, 0.017779605463147163, 0.017012035474181175, 0.015083014965057373, 0.021189020946621895, 0.029032999649643898, 0.009482881985604763, 0.020659223198890686, 0.03205469623208046, 0.15726503729820251, 0.16963164508342743, 0.18387484550476074, 0.047721803188323975], 'accuracy': [0.47857141494750977, 0.5571428537368774, 0.7142857313156128, 0.800000011920929, 0.7857142686843872, 0.8714285492897034, 0.9071428775787354, 0.9142857193946838, 0.9214285612106323, 0.9642857313156128, 0.9428571462631226, 0.949999988079071, 0.9785714149475098, 0.9857142567634583, 0.9714285731315613, 0.9571428298950195, 0.9857142567634583, 0.9857142567634583, 0.9857142567634583, 0.9928571581840515, 0.9785714149475098, 1.0, 0.9928571581840515, 0.9928571581840515, 1.0, 1.0, 0.9928571581840515, 0.9928571581840515, 1.0, 0.9928571581840515, 1.0, 0.9928571581840515, 0.9928571581840515, 0.9928571581840515, 0.9928571581840515, 0.9857142567634583, 0.9928571581840515, 1.0, 1.0, 0.9928571581840515, 1.0, 0.9928571581840515, 0.9928571581840515, 1.0, 0.9928571581840515, 0.9928571581840515, 0.9785714149475098, 0.9571428298950195, 0.9642857313156128, 0.9857142567634583], 'val_loss': [2.7767446041107178, 1.6069942712783813, 1.156929612159729, 1.4032578468322754, 0.9840988516807556, 0.9241390228271484, 1.616621971130371, 1.3568209409713745, 1.2460193634033203, 1.1465754508972168, 1.122255802154541, 1.1818928718566895, 1.1760294437408447, 0.8287503719329834, 0.8993058800697327, 1.1515263319015503, 1.1723254919052124, 1.132250428199768, 1.2064299583435059, 0.7634830474853516, 1.095198631286621, 1.1093966960906982, 1.0770611763000488, 1.1346157789230347, 1.1614599227905273, 1.5289111137390137, 1.5340476036071777, 1.1972432136535645, 1.4852606058120728, 1.4679116010665894, 1.466435194015503, 1.474334716796875, 1.5033353567123413, 1.4692178964614868, 1.5457550287246704, 1.4953217506408691, 1.4808335304260254, 1.5311532020568848, 1.9149284362792969, 1.7083845138549805, 1.5470890998840332, 1.5481981039047241, 1.5530245304107666, 1.6642000675201416, 1.997262954711914, 1.9683098793029785, 1.7448757886886597, 1.9288095235824585, 2.3099749088287354, 2.4272069931030273], 'val_accuracy': [0.5277777910232544, 0.5833333134651184, 0.5, 0.6111111044883728, 0.6388888955116272, 0.75, 0.8055555820465088, 0.75, 0.7222222089767456, 0.8055555820465088, 0.8333333134651184, 0.75, 0.7777777910232544, 0.75, 0.6944444179534912, 0.75, 0.7222222089767456, 0.7777777910232544, 0.8055555820465088, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8055555820465088, 0.8333333134651184, 0.7777777910232544, 0.75, 0.8055555820465088, 0.8055555820465088, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8055555820465088, 0.8055555820465088, 0.75, 0.75, 0.8333333134651184, 0.8333333134651184, 0.8055555820465088, 0.6944444179534912, 0.75, 0.8333333134651184, 0.8055555820465088, 0.6944444179534912, 0.6944444179534912, 0.6944444179534912, 0.6944444179534912, 0.75, 0.75, 0.75]}
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
bidirectional (Bidirectional multiple                  2230272
_________________________________________________________________
dropout (Dropout)            multiple                  0
_________________________________________________________________
lstm_1 (LSTM)                multiple                  36992
_________________________________________________________________
dropout_1 (Dropout)          multiple                  0
_________________________________________________________________
dense (Dense)                multiple                  33
=================================================================
Total params: 2,267,297
Trainable params: 2,267,297
Non-trainable params: 0
_________________________________________________________________

Process finished with exit code 0
